{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, random_split\nimport torchvision\nfrom torchvision.transforms import v2\nfrom torchvision.transforms.v2 import ToTensor\nfrom torchvision import datasets\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-13T16:25:35.891117Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T16:25:35.899380Z","iopub.execute_input":"2025-10-13T16:25:35.899740Z","iopub.status.idle":"2025-10-13T16:25:35.924278Z","shell.execute_reply.started":"2025-10-13T16:25:35.899708Z","shell.execute_reply":"2025-10-13T16:25:35.923251Z"}},"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"'cpu'"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"transform_train = v2.Compose([\n    v2.RandomHorizontalFlip(p=0.5),\n    v2.RandomCrop(32, padding=4),\n    v2.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    v2.RandomRotation(15),\n    ToTensor(),\n    v2.Normalize(\n        mean=[0.5071, 0.4867, 0.4408],\n        std=[0.2675, 0.2565, 0.2761]\n    )\n])\ntransform_test = v2.Compose([\n    ToTensor(),\n    v2.Normalize(\n        mean=[0.5071, 0.4867, 0.4408],\n        std=[0.2675, 0.2565, 0.2761]\n    )\n])\ntrainset = datasets.CIFAR100(root = './data', train = True, download= True, transform = transform_train)\ntestset = datasets.CIFAR100(root = './data', train = False, download= True, transform = transform_test)\n\ntrain_size = int(0.8* len(trainset))\nvalid_size = len(trainset) - train_size\ntrainset, validset = random_split(trainset, [train_size, valid_size])\n\ntrain_loader = DataLoader(trainset, batch_size = 64, shuffle = True)\nvalid_loader = DataLoader(validset, batch_size = 64, shuffle = False)\ntest_loader = DataLoader(testset, batch_size = 64, shuffle = False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_dataloader_model(\n    model, train_loader, valid_loader, optim, loss_fn,\n    device=None, max_epochs=100, diff=1e-3, patience=5\n):\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    model.to(device)\n\n    train_losses = []\n    valid_losses = []\n    valid_accuracies = []\n\n    valid_loss_min = float(\"inf\")\n    patience_counter = 0\n\n    for epoch in range(max_epochs):\n        # ----------------------------\n        # TRAINING PHASE\n        # ----------------------------\n        model.train()\n        running_train_loss = 0.0\n\n        for X_train, y_train in train_loader:\n            X_train, y_train = X_train.to(device), y_train.to(device)\n\n            optim.zero_grad()\n            y_pred = model(X_train)\n            loss = loss_fn(y_pred, y_train)\n            loss.backward()\n            optim.step()\n\n            running_train_loss += loss.item()\n\n        epoch_train_loss = running_train_loss / len(train_loader)\n        train_losses.append(epoch_train_loss)\n\n        # ----------------------------\n        # VALIDATION PHASE\n        # ----------------------------\n        model.eval()\n        running_valid_loss = 0.0\n        correct = 0\n        total = 0\n\n        with torch.no_grad():\n            for X_valid, y_valid in valid_loader:\n                X_valid, y_valid = X_valid.to(device), y_valid.to(device)\n                y_pred = model(X_valid)\n\n                # Compute validation loss\n                loss = loss_fn(y_pred, y_valid)\n                running_valid_loss += loss.item()\n\n                # Compute accuracy\n                _, predicted = torch.max(y_pred, 1)\n                correct += (predicted == y_valid).sum().item()\n                total += y_valid.size(0)\n\n        epoch_valid_loss = running_valid_loss / len(valid_loader)\n        epoch_valid_acc = correct / total\n\n        valid_losses.append(epoch_valid_loss)\n        valid_accuracies.append(epoch_valid_acc)\n\n        scheduler.step(epoch_valid_loss)\n        # ----------------------------\n        # EARLY STOPPING CHECKS\n        # ----------------------------\n        if epoch > 0 and abs(train_losses[-1] - train_losses[-2]) < diff:\n            print(f\"epoch {epoch}: loss diff < {diff} â†’ early convergence.\")\n            break\n\n        if epoch_valid_loss < valid_loss_min - diff:\n            valid_loss_min = epoch_valid_loss\n            patience_counter = 0\n        else:\n            patience_counter += 1\n\n        print(\n            f\"Epoch [{epoch+1}/{max_epochs}] | \"\n            f\"Train Loss: {epoch_train_loss:.4f} | \"\n            f\"Valid Loss: {epoch_valid_loss:.4f} | \"\n            f\"Valid Acc: {epoch_valid_acc*100:.2f}%\"\n        )\n\n        if patience_counter >= patience:\n            print(f\"\\nEarly stopping: no improvement for {patience} epochs.\")\n            break\n\n    # ----------------------------\n    # PLOTTING BEFORE RETURN\n    # ----------------------------\n    epochs = range(1, len(train_losses) + 1)\n\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, train_losses, label=\"Train Loss\")\n    plt.plot(epochs, valid_losses, label=\"Valid Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Training vs Validation Loss\")\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, valid_accuracies, label=\"Valid Accuracy\", color=\"green\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"Validation Accuracy\")\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n    return train_losses, valid_losses, valid_accuracies\nclass Residual(nn.Module):\n    def __init__(self, in_channels, out_channels, use_1x1conv=False, strides=1):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n                               padding=1, stride=strides)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n                               padding=1)\n        if use_1x1conv:\n            self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=1,\n                                   stride=strides)\n        else:\n            self.conv3 = None\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU()\n\n    def forward(self, X):\n        Y = self.relu(self.bn1(self.conv1(X)))\n        Y = self.bn2(self.conv2(Y))\n        if self.conv3:\n            X = self.conv3(X)\n        Y += X\n        return self.relu(Y)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ResNeXtBlock(nn.Module):  #@save\n    \"\"\"The ResNeXt block.\"\"\"\n    def __init__(self,in_channels, out_channels, groups, bot_mul, use_1x1conv=False,\n                 strides=1):\n        super().__init__()\n        bot_channels = int(round(out_channels * bot_mul))\n        self.conv1 = nn.Conv2d(in_channels, bot_channels, kernel_size=1, stride=1)\n        self.bn1 = nn.BatchNorm2d(bot_channels)\n        \n        self.conv2 = nn.Conv2d(bot_channels, bot_channels, kernel_size=3,\n                                   stride=strides, padding=1,\n                                   groups=bot_channels//groups)\n        self.bn2 = nn.BatchNorm2d(bot_channels)\n        \n        self.conv3 = nn.Conv2d(out_channels, kernel_size=1, stride=1)\n        self.bn3 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU()\n        if use_1x1conv:\n            self.conv4 = nn.Conv2d(in_channels, out_channels, kernel_size=1,\n                                       stride=strides)\n            self.bn4 = nn.BatchNorm2d(out_channels)\n        else:\n            self.conv4 = None\n\n    def forward(self, X):\n        Y = self.relu(self.bn1(self.conv1(X)))\n        Y = self.relu(self.bn2(self.conv2(Y)))\n        Y = self.bn3(self.conv3(Y))\n        if self.conv4:\n            X = self.bn4(self.conv4(X))\n        return self.relu(Y + X)\nclass ResNeXt(nn.Module):\n    def block(self, num_residuals, in_channel, out_channels,\n              groups=32, bot_mul=0.25, first_block=False):\n        blk = []\n        for i in range(num_residuals):\n            if i == 0 and not first_block:\n               blk.append(ResNeXtBlock(in_channel, out_channels,\n                                    groups, bot_mul, use_1x1conv=True, strides=2))\n            else:\n                blk.append(ResNeXtBlock(out_channels, out_channels,\n                                    groups, bot_mul))\n            in_channel = out_channels\n        return nn.Sequential(*blk)\n\n    def __init__(self, arch, num_classes=100):\n        super(ResNeXt, self).__init__()\n        self.b1 = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64), nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n        self.net = nn.Sequential(self.b1)\n        for i, b in enumerate(arch):\n            self.net.add_module(f'b{i+2}', self.block(*b, first_block=(i==0)))\n        self.net.add_module('last', nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n            nn.Linear(512, num_classes)))\n    def forward(self, x):\n        return self.net(x)\n\nclass ResNeXt18(ResNeXt):\n    def __init__(self, num_classes=100):\n        super().__init__(((2, 64, 64), (2, 64, 128), (2, 128, 256), (2, 256, 512))\n                         , num_classes)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = ResNeXt18().to(device)\noptim = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay = 1e-4)\nloss_fn = nn.CrossEntropyLoss()\nscheduler = ReduceLROnPlateau(optim, mode='min', factor=0.5, patience=3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataloader_model(model, train_loader, valid_loader, optim= optim, loss_fn= loss_fn)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}