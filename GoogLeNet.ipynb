{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMNuLuPhdKFnh1s2NXJIYiq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhChLong/ML-DL/blob/main/GoogLeNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "PvbD-AQv_4aX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torchvision\n",
        "from torchvision.transforms import v2, ToTensor\n",
        "from torchvision import datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "GM7l7NitVbF8"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "m-NE9oXYDhI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_img = v2.Compose(\n",
        "    [v2.Resize((96, 96)),\n",
        "    ToTensor(),\n",
        "    v2.Normalize(\n",
        "    mean=[0.5],\n",
        "    std=[0.5])]\n",
        ")\n",
        "trainset = datasets.FashionMNIST(root = './data', train = True, download= True, transform = transform_img)\n",
        "testset = datasets.FashionMNIST(root = './data', train = False, download= True, transform = transform_img)\n",
        "\n",
        "train_size = int(0.8* len(trainset))\n",
        "valid_size = len(trainset) - train_size\n",
        "trainset, validset = random_split(trainset, [train_size, valid_size])\n",
        "\n",
        "train_loader = DataLoader(trainset, batch_size = 64, shuffle = True)\n",
        "valid_loader = DataLoader(validset, batch_size = 64, shuffle = False)\n",
        "test_loader = DataLoader(testset, batch_size = 64, shuffle = False)"
      ],
      "metadata": {
        "id": "it6ux8yZDSIs"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GoogLeNet"
      ],
      "metadata": {
        "id": "x5XxzDgcHryI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inception Block"
      ],
      "metadata": {
        "id": "J6Js1Tz7Of6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InceptionBlock(nn.Module):\n",
        "    def __init__(self, in_chan, out1, out2, out3, out4):\n",
        "        super().__init__()\n",
        "        self.b1 = nn.Sequential(\n",
        "            nn.Conv2d(in_chan, out1, kernel_size=1), nn.BatchNorm2d(out1), nn.ReLU()\n",
        "        )\n",
        "        self.b2 = nn.Sequential(\n",
        "            nn.Conv2d(in_chan, out2[0], kernel_size=1), nn.BatchNorm2d(out2[0]), nn.ReLU(),\n",
        "            nn.Conv2d(out2[0],out2[1], kernel_size= 3, padding= 1), nn.BatchNorm2d(out2[1]), nn.ReLU()\n",
        "        )\n",
        "        self.b3 = nn.Sequential(\n",
        "            nn.Conv2d(in_chan, out3[0], kernel_size=1) ,nn.BatchNorm2d(out3[0]), nn.ReLU(),\n",
        "            nn.Conv2d(out3[0],out3[1], kernel_size= 5, padding= 2),nn.BatchNorm2d(out3[1]), nn.ReLU()\n",
        "        )\n",
        "        self.b4 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=3, stride = 1, padding = 1),\n",
        "            nn.Conv2d(in_chan, out4, kernel_size=1),nn.BatchNorm2d(out4), nn.ReLU()\n",
        "        )\n",
        "        self.out_channel = out1 + out2[1] + out3[1] + out4\n",
        "    def forward(self, x):\n",
        "        x1 = self.b1(x)\n",
        "        x2 = self.b2(x)\n",
        "        x3 = self.b3(x)\n",
        "        x4 = self.b4(x)\n",
        "        return torch.concat((x1, x2, x3, x4), dim = 1)"
      ],
      "metadata": {
        "id": "H2yncsTyHwXM"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "JL-kUCxBOkFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GoogLeNet(nn.Module):\n",
        "    def __init__(self, classes = 10):\n",
        "        super().__init__()\n",
        "        self.b1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
        "        self.b2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=1),  nn.ReLU(),\n",
        "            nn.Conv2d(64, 192, kernel_size=3, padding=1), nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
        "        self.b3 = nn.Sequential(InceptionBlock(192, 64, (96, 128), (16, 32), 32),\n",
        "                         InceptionBlock(256, 128, (128, 192), (32, 96), 64),\n",
        "                         nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
        "        ## num_output_channel = 256 + 480 = 736\n",
        "        self.b4 = nn.Sequential(InceptionBlock(480, 192, (96, 208), (16, 48), 64),\n",
        "                         InceptionBlock(512, 160, (112, 224), (24, 64), 64),\n",
        "                         InceptionBlock(512, 128, (128, 256), (24, 64), 64),\n",
        "                         InceptionBlock(512, 112, (144, 288), (32, 64), 64),\n",
        "                         InceptionBlock(528, 256, (160, 320), (32, 128), 128),\n",
        "                         nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
        "        self.b5 = nn.Sequential(InceptionBlock(832, 256, (160, 320), (32, 128), 128),\n",
        "                         InceptionBlock(832, 384, (192, 384), (48, 128), 128),\n",
        "                         nn.AdaptiveAvgPool2d((1,1)), nn.Flatten())\n",
        "        self.net = nn.Sequential(\n",
        "            self.b1, self.b2, self.b3, self.b4, self.b5, nn.Linear(1024, 10)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "YvGGc031OmRa"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "bkjslxwkVRvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dataloader_model(model, train_loader, valid_loader, optim, loss_fn, device = None, max_epoches = 100, diff = 1e-3, patience = 10):\n",
        "    if device is None:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    train_losses = []\n",
        "    valid_loss_min = float(\"inf\")\n",
        "    patience_counter = 0\n",
        "    for epoch in range(max_epoches):\n",
        "        epoch_loss = 0\n",
        "        model.train()\n",
        "        for X_train, y_train in train_loader:\n",
        "            X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "\n",
        "            y_train_pred = model(X_train)\n",
        "            loss = loss_fn(y_train_pred, y_train)\n",
        "            optim.zero_grad()\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        train_losses.append(epoch_loss/len(train_loader))\n",
        "\n",
        "        model.eval()\n",
        "        if epoch > 0:\n",
        "            if abs(train_losses[-2] - train_losses[-1]) < diff:\n",
        "                print(f\"epoch: {epoch}\\t|| loss: {epoch_loss/len(train_loader):.4f}\")\n",
        "                print(\"break due to model converges\")\n",
        "                return\n",
        "        with torch.no_grad():\n",
        "            epoch_valid_loss = 0\n",
        "            for X_valid, y_valid in valid_loader:\n",
        "                X_valid, y_valid = X_valid.to(device), y_valid.to(device)\n",
        "                y_valid_pred = model(X_valid)\n",
        "                c_loss = loss_fn(y_valid_pred, y_valid)\n",
        "                epoch_valid_loss += c_loss.item()\n",
        "            epoch_valid_loss /= len(valid_loader)\n",
        "        if epoch_valid_loss < valid_loss_min - diff:\n",
        "            valid_loss_min = epoch_valid_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"epoch: {epoch} || train_loss: {epoch_loss/len(train_loader):.4f} || valid_loss: {epoch_valid_loss:.4f}\")\n",
        "            print(f\"Early stopping - no improvement for {patience} epochs\")\n",
        "            return\n",
        "        # if epoch % 2 == 0:\n",
        "        print(f\"epoch: {epoch}\\t|| loss: {epoch_loss/len(train_loader):.4f}\\t || valid_loss: {epoch_valid_loss:.4f}\")"
      ],
      "metadata": {
        "id": "qq_m_nFvVTA3"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GoogLeNet().to(device)\n",
        "optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "wNkfK_DnVWIS"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader_model(model, train_loader, valid_loader, optim= optim, loss_fn= loss_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "G8sfFhsLV8-J",
        "outputId": "59149cb1-b342-4a1f-cea4-514236fff29c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0\t|| loss: 0.4739\t || valid_loss: 0.3748\n",
            "epoch: 1\t|| loss: 0.3150\t || valid_loss: 0.3047\n",
            "epoch: 2\t|| loss: 0.2660\t || valid_loss: 0.2616\n",
            "epoch: 3\t|| loss: 0.2406\t || valid_loss: 0.2392\n",
            "epoch: 4\t|| loss: 0.2160\t || valid_loss: 0.2272\n",
            "epoch: 5\t|| loss: 0.2011\t || valid_loss: 0.2282\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3863675027.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataloader_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1070203597.py\u001b[0m in \u001b[0;36mtrain_dataloader_model\u001b[0;34m(model, train_loader, valid_loader, optim, loss_fn, device, max_epoches, diff, patience)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}