{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, random_split\nimport torchvision\nfrom torchvision.transforms import v2\nfrom torchvision.transforms.v2 import ToTensor\nfrom torchvision import datasets\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transform_train = v2.Compose([\n    v2.RandomHorizontalFlip(p=0.5),\n    v2.RandomCrop(32, padding=4),\n    v2.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    v2.RandomRotation(15),\n    ToTensor(),\n    v2.Normalize(\n        mean=[0.5071, 0.4867, 0.4408],\n        std=[0.2675, 0.2565, 0.2761]\n    )\n])\ntransform_test = v2.Compose([\n    ToTensor(),\n    v2.Normalize(\n        mean=[0.5071, 0.4867, 0.4408],\n        std=[0.2675, 0.2565, 0.2761]\n    )\n])\ntrainset = datasets.CIFAR100(root = './data', train = True, download= True, transform = transform_train)\ntestset = datasets.CIFAR100(root = './data', train = False, download= True, transform = transform_test)\n\ntrain_size = int(0.8* len(trainset))\nvalid_size = len(trainset) - train_size\ntrainset, validset = random_split(trainset, [train_size, valid_size])\n\ntrain_loader = DataLoader(trainset, batch_size = 64, shuffle = True)\nvalid_loader = DataLoader(validset, batch_size = 64, shuffle = False)\ntest_loader = DataLoader(testset, batch_size = 64, shuffle = False)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_dataloader_model(\n    model, train_loader, valid_loader, optim, loss_fn,\n    device=None, max_epochs=100, diff=1e-3, patience=5\n):\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    model.to(device)\n\n    train_losses = []\n    valid_losses = []\n    valid_accuracies = []\n\n    valid_loss_min = float(\"inf\")\n    patience_counter = 0\n\n    for epoch in range(max_epochs):\n        # ----------------------------\n        # TRAINING PHASE\n        # ----------------------------\n        model.train()\n        running_train_loss = 0.0\n\n        for X_train, y_train in train_loader:\n            X_train, y_train = X_train.to(device), y_train.to(device)\n\n            optim.zero_grad()\n            y_pred = model(X_train)\n            loss = loss_fn(y_pred, y_train)\n            loss.backward()\n            optim.step()\n\n            running_train_loss += loss.item()\n\n        epoch_train_loss = running_train_loss / len(train_loader)\n        train_losses.append(epoch_train_loss)\n\n        # ----------------------------\n        # VALIDATION PHASE\n        # ----------------------------\n        model.eval()\n        running_valid_loss = 0.0\n        correct = 0\n        total = 0\n\n        with torch.no_grad():\n            for X_valid, y_valid in valid_loader:\n                X_valid, y_valid = X_valid.to(device), y_valid.to(device)\n                y_pred = model(X_valid)\n\n                # Compute validation loss\n                loss = loss_fn(y_pred, y_valid)\n                running_valid_loss += loss.item()\n\n                # Compute accuracy\n                _, predicted = torch.max(y_pred, 1)\n                correct += (predicted == y_valid).sum().item()\n                total += y_valid.size(0)\n\n        epoch_valid_loss = running_valid_loss / len(valid_loader)\n        epoch_valid_acc = correct / total\n\n        valid_losses.append(epoch_valid_loss)\n        valid_accuracies.append(epoch_valid_acc)\n\n        scheduler.step(epoch_valid_loss)\n        # ----------------------------\n        # EARLY STOPPING CHECKS\n        # ----------------------------\n        if epoch > 0 and abs(train_losses[-1] - train_losses[-2]) < diff:\n            print(f\"epoch {epoch}: loss diff < {diff} â†’ early convergence.\")\n            break\n\n        if epoch_valid_loss < valid_loss_min - diff:\n            valid_loss_min = epoch_valid_loss\n            patience_counter = 0\n        else:\n            patience_counter += 1\n\n        print(\n            f\"Epoch [{epoch+1}/{max_epochs}] | \"\n            f\"Train Loss: {epoch_train_loss:.4f} | \"\n            f\"Valid Loss: {epoch_valid_loss:.4f} | \"\n            f\"Valid Acc: {epoch_valid_acc*100:.2f}%\"\n        )\n\n        if patience_counter >= patience:\n            print(f\"\\nEarly stopping: no improvement for {patience} epochs.\")\n            break\n\n    # ----------------------------\n    # PLOTTING BEFORE RETURN\n    # ----------------------------\n    epochs = range(1, len(train_losses) + 1)\n\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, train_losses, label=\"Train Loss\")\n    plt.plot(epochs, valid_losses, label=\"Valid Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Training vs Validation Loss\")\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, valid_accuracies, label=\"Valid Accuracy\", color=\"green\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"Validation Accuracy\")\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n    return train_losses, valid_losses, valid_accuracies\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def conv_block(in_channels, out_channels):\n    return nn.Sequential(\n        nn.BatchNorm2d(in_channels), nn.ReLU(),\n        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias = False))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DenseBlock(nn.Module):\n    def __init__(self, num_convs, num_channels, growth_rate):\n        super(DenseBlock, self).__init__()\n        layer = []\n        for i in range(num_convs):\n            layer.append(conv_block(num_channels + i * growth_rate, growth_rate))\n        self.net = nn.Sequential(*layer)\n\n    def forward(self, X):\n        for blk in self.net:\n            Y = blk(X)\n            # Concatenate input and output of each block along the channels\n            X = torch.cat((X, Y), dim=1)\n        return X","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def transition_block(in_channels, out_channels):\n    return nn.Sequential(\n        nn.BatchNorm2d(in_channels), nn.ReLU(),\n        nn.Conv2d(in_channels, out_channels, kernel_size=1),\n        nn.AvgPool2d(kernel_size=2, stride=2))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DenseNet(nn.Module):\n    def b1(self):\n        return nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64), nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n    def __init__(self, num_channels=64, growth_rate=32, arch=(4, 4, 4, 4),\n            num_classes=100):\n        super(DenseNet, self).__init__()\n        self.net = nn.Sequential(self.b1())\n        for i, num_convs in enumerate(arch):\n            self.net.add_module(f'dense_blk{i+1}', DenseBlock(num_convs, num_channels,\n                                                              growth_rate))\n            # The number of output channels in the previous dense block\n            num_channels += num_convs * growth_rate\n            # A transition layer that halves the number of channels is added\n            # between the dense blocks\n            if i != len(arch) - 1:\n                \n                self.net.add_module(f'tran_blk{i+1}', transition_block(num_channels,\n                    num_channels//2))\n                num_channels //= 2\n        self.net.add_module('last', nn.Sequential(\n            nn.BatchNorm2d(num_channels), nn.ReLU(),\n            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n            nn.Linear(num_channels, num_classes)))\n    def forward(self, x):\n        return self.net(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = DenseNet().to(device)\noptim = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay = 1e-4)\nloss_fn = nn.CrossEntropyLoss()\nscheduler = ReduceLROnPlateau(optim, mode='min', factor=0.5, patience=3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataloader_model(model, train_loader, valid_loader, optim= optim, loss_fn= loss_fn)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}